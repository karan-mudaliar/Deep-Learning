{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0ff28b42",
      "metadata": {
        "id": "0ff28b42"
      },
      "source": [
        "This can be run [run on Google Colab using this link](https://colab.research.google.com/github/CS7150/CS7150-Homework-2/blob/main/HW2_2_CIFAR10Classifier.ipynb)\n",
        "\n",
        "<font size='6'>**Homework 2.2: Neural Network CIFAR-10 Classification**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef1d8e6",
      "metadata": {
        "id": "0ef1d8e6"
      },
      "source": [
        "<font size='5'>**Overview**</font>\n",
        "\n",
        "In this CS7150 assignment, our objective is to build a neural network featuring two fully-connected layers designed for classification purposes. We will evaluate the performance of this neural network by testing it on the CIFAR-10 dataset.\n",
        "\n",
        "This assignment adheres to a standard classification setup, which encompasses the use of a dataloader to load labeled image data in a natural form and training the model in a minibatch-based fashion.\n",
        "\n",
        "**Your assignment**: Your responsibility throughout this notebook is to thoroughly review the content and address all the conceptual and technical questions identified within the sections marked with \"Task\" headers and \"TODO:\" comments in the code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ih2KqUX1Aj1G",
      "metadata": {
        "id": "ih2KqUX1Aj1G"
      },
      "source": [
        "<font size='5'>**I) Setup**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bGxnahJAlmm",
      "metadata": {
        "id": "1bGxnahJAlmm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "075f340c",
      "metadata": {
        "id": "075f340c"
      },
      "source": [
        "<font size='4'>**Device Setup**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33798935",
      "metadata": {
        "id": "33798935"
      },
      "source": [
        "\n",
        "We aim to enable model training on a GPU to expedite our computations. First, we'll check whether torch.cuda is accessible; if it is, we will utilize the GPU; otherwise, we will continue to using the CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9a41951",
      "metadata": {
        "id": "c9a41951"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d6db073",
      "metadata": {
        "id": "2d6db073"
      },
      "source": [
        "<font size='4'>**Loading CIFAR-10 Data**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af842fd2",
      "metadata": {
        "id": "af842fd2"
      },
      "source": [
        "The CIFAR-10 dataset comprises a collection of 60,000 32x32 color images distributed across ten distinct classes. These classes correspond to various objects and include airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Within each class, there are precisely 6,000 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9c37c7",
      "metadata": {
        "id": "0c9c37c7"
      },
      "outputs": [],
      "source": [
        "# downloading cifar10 into folder\n",
        "data_dir = 'cifar10_data' # make sure that this folder is created in your working dir\n",
        "\n",
        "#TODO: Fill out train_data and test_data variables using CIFAR10 (i.e., torchvision.datasets.CIFAR)\n",
        "train_data = CIFAR10(data_dir, train=True, download=True, transform=Compose([ToTensor()]))\n",
        "test_data = CIFAR10(data_dir, train=False, download=True, transform=Compose([ToTensor()]))\n",
        "#train_data = None\n",
        "#test_data = None\n",
        "print(f'Datatype of the dataset object: {type(train_data)}')\n",
        "# check the length of dataset\n",
        "print(f'Number of samples in training data: {len(train_data)}')\n",
        "print(f'Number of samples in test data: {len(test_data)}')\n",
        "# Check the format of dataset\n",
        "print(f'Format of the dataset: \\n {train_data}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "507b2b64",
      "metadata": {
        "id": "507b2b64"
      },
      "source": [
        "### <font size='4'>**Displaying Loaded Dataset**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b496c9",
      "metadata": {
        "id": "22b496c9"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2, 3, i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(train_data[i][0][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Class Label: {}\".format(train_data[i][1]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b11341",
      "metadata": {
        "id": "f4b11341"
      },
      "source": [
        "## <font size='5'>**II) Building a Neural Network**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a180f9c7",
      "metadata": {
        "id": "a180f9c7"
      },
      "source": [
        "### <font size='4'>**1) Defining `CIFAR10Classifier` class**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003e4e25",
      "metadata": {
        "id": "003e4e25"
      },
      "source": [
        "<font size='4' color='Red'>Task 1.1 - Defining `CIFAR10Classifier` class (4 points)</font>\n",
        "\n",
        "In the following class, make adjustments to the following attributes: flatten, hidden_size, class_size, and linear_relu_stack. Ensure that the linear_relu_stack consists of a minimum of two linear layers combined with a non-linear activation layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd2c5289",
      "metadata": {
        "id": "bd2c5289"
      },
      "outputs": [],
      "source": [
        "class CIFAR10Classifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Classifier, self).__init__()\n",
        "        ########################################################################\n",
        "        # TODO: Complete the following variables as instructed earlier\n",
        "        ########################################################################\n",
        "        self.flatten = None\n",
        "        self.hidden_size = None\n",
        "        self.class_size = None\n",
        "        self.linear_relu_stack = None\n",
        "        ########################################################################\n",
        "        #                             END OF YOUR CODE                         #\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "720bfbd5",
      "metadata": {
        "id": "720bfbd5"
      },
      "source": [
        "### <font size='4'>**2) Training a Neural Network**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fU8M2hZKKIqf",
      "metadata": {
        "id": "fU8M2hZKKIqf"
      },
      "source": [
        "<font size='4' color='Red'>Task 1.2 - Defining parameters (3 points)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nWQyEYmrDSV3",
      "metadata": {
        "id": "nWQyEYmrDSV3"
      },
      "source": [
        "Let's create an instance of `CIFAR10Classifier` and move it to the device. After doing so, we define the following hyperparameters for training:\n",
        "\n",
        "- **Number of Epochs**: This signifies the number of iterations over the dataset.\n",
        "- **Batch Size**: It represents the number of data samples that propagate through the network before parameter updates.\n",
        "- **Learning Rate**: This parameter determines the extent of model parameter updates during each batch/epoch. Smaller values lead to slower learning, while larger values may introduce instability during training.\n",
        "\n",
        "**Your Task**:\n",
        "\n",
        "1. Set `learning_rate` to 1e-3, `batch_size` to 64, and `epochs` to 10 initially. Experiment with different values and retain the final choices that yield the highest testing accuracy.\n",
        "\n",
        "2. Select an appropriate loss function. You should experiment with different options, such as `CrossEntropyLoss()`, `MSELoss()`, and any others, and choose the one that best suits the task.\n",
        "\n",
        "3. Define the `optimizer` variable using any optimizer function (e.g., SGD, Adam, etc.). Be sure to explore different parameter values within the chosen optimizer function.\n",
        "\n",
        "4. Remember to record your ultimate choices for each variable that contribute to achieving the best performance for your `CIFAR10Classifier`. To receive full credit for this assignment, your model should attain a classification accuracy of over 50% on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WSTgaNxI9c5f",
      "metadata": {
        "id": "WSTgaNxI9c5f"
      },
      "outputs": [],
      "source": [
        "model = CIFAR10Classifier().to(device)\n",
        "model.requires_grad_(True)\n",
        "\n",
        "########################################################################\n",
        "# TODO: Complete the following variables as instructed earlier\n",
        "########################################################################\n",
        "\n",
        "learning_rate = None\n",
        "batch_size = None\n",
        "epochs = None\n",
        "loss_fn = None\n",
        "optimizer = None\n",
        "\n",
        "########################################################################\n",
        "#                             END OF YOUR CODE                         #\n",
        "########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556ce020",
      "metadata": {
        "id": "556ce020"
      },
      "source": [
        "### <font size='4'>**3) Train Loop**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf5e0b8",
      "metadata": {
        "id": "0cf5e0b8"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a43130c9",
      "metadata": {
        "id": "a43130c9"
      },
      "source": [
        "In the above cell, we used a Dataloader to create batches for training and testing data. For each batch of size indicated in the batch_size hyperparameter, we perform backprop and update the model parameters' weights and biases.\n",
        "\n",
        "In the following cell, we define our train_loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4eeed5",
      "metadata": {
        "id": "fa4eeed5"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, print_log=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    correct = 0\n",
        "    training_acc = 0\n",
        "    training_loss = 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X.to(device))\n",
        "        loss = loss_fn(pred, y.to(device))\n",
        "        correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (print_log==True) and (batch % 100 == 0):\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            training_loss = loss\n",
        "            print(f\"\"\"Training loop: loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\"\"\")\n",
        "    if (print_log==True):\n",
        "        correct /= size\n",
        "        training_acc = 100*correct\n",
        "        print(f\"\"\"Training Accuracy: {training_acc:>0.1f}%\"\"\")\n",
        "    return training_acc, training_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b6072ee",
      "metadata": {
        "id": "2b6072ee"
      },
      "source": [
        "### <font size='4'>**4) Test Loop**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba22bb45",
      "metadata": {
        "id": "ba22bb45"
      },
      "source": [
        "In the test loop, we iterate over the test dataset to check if model performance is improving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a05d6353",
      "metadata": {
        "id": "a05d6353"
      },
      "outputs": [],
      "source": [
        "def test_loop(dataloader, model, loss_fn, print_log=True):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X.to(device))\n",
        "            test_loss += loss_fn(pred, y.to(device)).item()\n",
        "            correct += (pred.argmax(1) == y.to(device)).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    testing_acc = 100*correct\n",
        "    if (print_log==True):\n",
        "        print(f\"Testing Accuracy: {testing_acc:>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return testing_acc, test_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10b7c82",
      "metadata": {
        "id": "d10b7c82"
      },
      "source": [
        "### <font size='4'>**5) Running the loops**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4245bd36",
      "metadata": {
        "id": "4245bd36"
      },
      "source": [
        "We run our loops for a certain number of times, which is indicated in the 'epoch' hyperparameter that we defined earlier. In the following cell, we run both our training and testing loop to see how our training and testing accuracies change over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "836fc7f0",
      "metadata": {
        "id": "836fc7f0"
      },
      "outputs": [],
      "source": [
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d2ca7b1",
      "metadata": {
        "id": "9d2ca7b1"
      },
      "source": [
        "## <font size='5'>**III) Fine Tuning Hyperparameters**</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vtmexjmGFYTR",
      "metadata": {
        "id": "vtmexjmGFYTR"
      },
      "source": [
        "Adjusting the hyperparameters and gaining a deeper understanding of how they impact the ultimate performance is a substantial aspect of working with neural networks. Therefore, we encourage you to gain practical experience in this regard."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f79ae32",
      "metadata": {
        "id": "7f79ae32"
      },
      "source": [
        "In this task, your goal is to play around with different settings for various options like layer size, batch size, learning rate. You should also experiment with optimizer hyperparameters including momentum, weight decay and more.\n",
        "\n",
        "To understand how these choices affect your model's performance, you'll create at least three graphs. Each graph will show how changing one of these options (except for epochs) impacts how well your model learns and predicts."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x719wW9FIynX",
      "metadata": {
        "id": "x719wW9FIynX"
      },
      "source": [
        "## <font size='4'>**Example** - We've given you an example code for changing number of epochs so you can see how it's done.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "olcxRr7T8bpj",
      "metadata": {
        "id": "olcxRr7T8bpj"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = [1, 5, 10, 15, 20, 25]\n",
        "momentum = 0\n",
        "weight_decay = 0\n",
        "dampening = 0\n",
        "\n",
        "# Train and Test\n",
        "test_accs = []\n",
        "test_losses = []\n",
        "training_accs = []\n",
        "for e in epochs: #Would change this to reflect whatever hyperparameter you would be testing\n",
        "    # Model\n",
        "    model = CIFAR10Classifier().to(device)\n",
        "    model.requires_grad_(True)\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.SGD(model.parameters(),\n",
        "                            lr = learning_rate,\n",
        "                            momentum = momentum,\n",
        "                            weight_decay = weight_decay,\n",
        "                            dampening= dampening)\n",
        "    # Loss Func\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # Dataloaders\n",
        "    train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
        "    test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "    final_train_acc = 0\n",
        "    final_test_acc = 0\n",
        "    final_test_loss = 0\n",
        "    for t in range(e):\n",
        "        # print(f\"Currently running epoch {t+1}\")\n",
        "        training_acc = train_loop(train_dataloader, model, loss_fn, optimizer, print_log=False)\n",
        "        testing_acc, test_loss =  test_loop(test_dataloader, model, loss_fn, print_log=False)\n",
        "        final_test_acc = testing_acc\n",
        "        final_test_loss = test_loss\n",
        "        final_train_acc = training_acc\n",
        "    test_accs.append(final_test_acc)\n",
        "    test_losses.append(final_test_loss)\n",
        "    training_accs.append(final_train_acc)\n",
        "plt.plot(epochs,test_losses, color ='tab:red', label='testing loss')\n",
        "plt.plot(epochs,test_accs, color ='tab:blue', label='testing accuracy')\n",
        "plt.plot(epochs,training_accs, color ='tab:green', label='training accuracy')\n",
        "plt.legend()\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w7Zi1npqHU7B",
      "metadata": {
        "id": "w7Zi1npqHU7B"
      },
      "source": [
        "<font size='4' color='Red'>Task 1.3 - Experiment 1 (2 point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd5IJznoH60y",
      "metadata": {
        "id": "cd5IJznoH60y"
      },
      "source": [
        "$$\\text{I am tuning ______________ hyperparameter for better performance}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Fkd6usOHP0c",
      "metadata": {
        "id": "9Fkd6usOHP0c"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# TODO: Implement your code here\n",
        "############################################################################\n",
        "raise NotImplementedError\n",
        "\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qnsKis_dLoGW",
      "metadata": {
        "id": "qnsKis_dLoGW"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bCXoR1dtIVqE",
      "metadata": {
        "id": "bCXoR1dtIVqE"
      },
      "source": [
        "<font size='4' color='Red'>Task 1.3 - Experiment 2 (2 point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NBR25sGzIaqE",
      "metadata": {
        "id": "NBR25sGzIaqE"
      },
      "source": [
        "$$\\text{I am tuning ______________ hyperparameter for better performance}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7Xm_gZhsIaqP",
      "metadata": {
        "id": "7Xm_gZhsIaqP"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# TODO: Implement your code here\n",
        "############################################################################\n",
        "raise NotImplementedError\n",
        "\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RXVjjad0IhZs",
      "metadata": {
        "id": "RXVjjad0IhZs"
      },
      "source": [
        "<font size='4' color='Red'>Task 1.4 - Experiment 3 (2 point)</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6NZyqrBXIhZs",
      "metadata": {
        "id": "6NZyqrBXIhZs"
      },
      "source": [
        "$$\\text{I am tuning ______________ hyperparameter for better performance}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XytrrMb3IhZs",
      "metadata": {
        "id": "XytrrMb3IhZs"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# TODO: Implement your code here\n",
        "############################################################################\n",
        "raise NotImplementedError\n",
        "\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}